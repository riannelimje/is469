{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "\n",
    "model_name = \"bert-base-cased\" # cased means Upper and Lower case are distinguished\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d31368",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello the time now is 1020 in the morning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a81f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens['input_ids'])\n",
    "# 101 - CLS, 102 - SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a826b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens3 = tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
    "\n",
    "print(f\"Tokens: {tokens3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fedc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc07fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd505c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Access the word embeddings within the BertEmbeddings layer\n",
    "### as opposed to the entire embedding layer that also has\n",
    "### position embeddings and token type embeddings\n",
    "word_embeddings = embedding_layer.word_embeddings\n",
    "\n",
    "print(word_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'pig'\n",
    "print(tokenizer(word1)) # 3 tokens because of CLS and SEP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2 = 'little'\n",
    "print(tokenizer(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids1 = tokenizer.encode(word1, add_special_tokens=False)\n",
    "token_ids2 = tokenizer.encode(word2, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8833cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-select embeddings from embeddings layer without the extra tokens this time\n",
    "embedding1 = embedding_layer.word_embeddings.weight[token_ids1]\n",
    "embedding2 = embedding_layer.word_embeddings.weight[token_ids2]\n",
    "\n",
    "# Convert tensors to NumPy arrays and squeeze result to remove extra dimensions\n",
    "embedding1 = embedding1.squeeze().detach().numpy()\n",
    "embedding2 = embedding2.squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norms of the embeddings\n",
    "magnitude1 = np.linalg.norm(embedding1)\n",
    "magnitude2 = np.linalg.norm(embedding2)\n",
    "\n",
    "cosine_sim = np.dot(embedding1, embedding2) / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007be456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cosine sim of '{word1}' and '{word2}': {cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b072c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn to compute cosine similarity - it's the same as above \n",
    "cosine_sim = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "print(f\"Cosine sim of '{word1}' and '{word2}': {cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de999a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Now we need the full output of the tokeniser for the forward pass, not just encode\n",
    "\n",
    "tokenids1 = tokenizer(word1, return_tensors=\"pt\", add_special_tokens=False)\n",
    "tokenids2 = tokenizer(word2, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Get embeddings from BERT model forward pass, do not store values needed for gradient comp (no_grad)\n",
    "# Now use token_ids1 and 2 since we need all compunents of the embeddings\n",
    "\n",
    "with torch.no_grad():\n",
    "    # forward pass only -- not doing any backward pass (training)\n",
    "    outputs1 = model(**tokenids1)\n",
    "    outputs2 = model(**tokenids2)\n",
    "\n",
    "# Extract embeddings for first token (word embedding)\n",
    "# The last_hidden_state is of shape (batch_size, sequence_length, hidden_dim)\n",
    "# Use average pooling over the sequence dimension (dim=1) to get a fixed-size embedding\n",
    "\n",
    "embedding1 = outputs1.last_hidden_state.mean(dim=1).squeeze().numpy() # if there are multiple tokens, we take the mean of the tokens\n",
    "embedding2 = outputs2.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "cosine_sim = cosine_similarity([embedding1], [embedding2])\n",
    "print(f\"cosine similarity of {word1} and {word2} is {cosine_sim}\") # the pretrained BERT weights modifies it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"hello i don't want homework\"\n",
    "sent2 = \"hello i don't want homework\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d008c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sent1 = tokenizer.encode(sent1)\n",
    "tokens_sent2 = tokenizer.encode(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embedding1 = embedding_layer.word_embeddings.weight[tokens_sent1].mean(dim=0).squeeze().detach()\n",
    "s_embedding2 = embedding_layer.word_embeddings.weight[tokens_sent2].mean(dim=0).squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559307b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_static = cosine_similarity([s_embedding1], [s_embedding2])\n",
    "print(f\"cosine similarity of '{sent1}' and '{sent2}' is {cosine_sim_static}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids1 = tokenizer(sent1, return_tensors=\"pt\", padding=True)\n",
    "## pad so that 2 sentences same length in tokens -\n",
    "#  as we will be passing them through the model as a BATCH]\n",
    "sent_ids2 = tokenizer(sent2, return_tensors=\"pt\", padding=True)\n",
    "## Get embeddings from BERT model forward pass, do not store values needed for gradient comp (no_grad)\n",
    "with torch.no_grad():\n",
    "    # Forward pass to get embeddings from the last hidden state\n",
    "    outputs1 = model(**sent_ids1)\n",
    "    outputs2 = model(**sent_ids2)\n",
    "\n",
    "s_embedding3 = outputs1.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "s_embedding4 = outputs2.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "cosine_sim_mean = cosine_similarity([s_embedding3], [s_embedding4])\n",
    "\n",
    "print(f\"Cosine sim with BERT forward is {cosine_sim_mean} \\n\"\n",
    "      f\"With STATIC embeddings it was {cosine_sim_static}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d73242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for  [CLS] token (index 0 of last_hidden_state)\n",
    "# The `last_hidden_state` has shape (batch_size, sequence_length, hidden_dim)\n",
    "embedding5 = outputs1.last_hidden_state[0, 0].numpy()  # [CLS] token for sent1\n",
    "embedding6 = outputs2.last_hidden_state[0, 0].numpy()\n",
    "\n",
    "# Now let's take their cosine sim as before\n",
    "\n",
    "cosine_sim_CLS = cosine_similarity([embedding5], [embedding6])\n",
    "\n",
    "print(f\"Cosine sim of [CLS] tokens is  {cosine_sim_CLS} \\n\"\n",
    "f\"Cosine sim with mean pooling is {cosine_sim_mean} \\n\"\n",
    "f\"and STATIC embeddings was {cosine_sim_static}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
