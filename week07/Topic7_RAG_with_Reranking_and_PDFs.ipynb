{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b35U0SzsOPn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mxRO4LU_MvZC",
        "outputId": "92584b24-1355-4ce0-a3f6-ea76afbce7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.31.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m844.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kDV6-F3OQNX",
        "outputId": "a13b12ee-d535-4e12-910d-16e8f237c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDrakf1RMqL6"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Function to split the text into chunks\n",
        "def split_into_chunks(text, max_chunk_size=200):\n",
        "  \"\"\"Splits the text into chunks of a maximum size.\"\"\"\n",
        "  chunks = []\n",
        "  current_chunk = \"\"\n",
        "  for sentence in text.split('.'):  # Split by sentences\n",
        "    if len(current_chunk) + len(sentence) < max_chunk_size:\n",
        "      current_chunk += sentence + '.'  # Add sentence to current chunk\n",
        "    else:\n",
        "      chunks.append(current_chunk.strip())  # Add current chunk to list\n",
        "      current_chunk = sentence + '.'  # Start a new chunk\n",
        "  chunks.append(current_chunk.strip())  # Add the last chunk\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "folder_path = '/content/drive/My Drive/PDFS'\n",
        "all_chunks = []  # List to store chunks\n",
        "document_sources = []  # List to store source PDF for each chunk, if multiple PDFs"
      ],
      "metadata": {
        "id": "kmMyVdpzvjDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(folder_path):\n",
        "  if filename.endswith('.pdf'):\n",
        "    pdf_path = os.path.join(folder_path, filename)\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "      pdf_reader = PyPDF2.PdfReader(file)\n",
        "      pdf_text = \"\"\n",
        "      for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        pdf_text += page.extract_text()\n",
        "\n",
        "      # Split PDF  into chunks\n",
        "      chunks = split_into_chunks(pdf_text)\n",
        "      all_chunks.extend(chunks)  # Add chunks to list\n",
        "      document_sources.extend([filename] * len(chunks))  # Store source doc"
      ],
      "metadata": {
        "id": "LsEru-Z1vp7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed chunks\n",
        "chunk_embeddings = model.encode(all_chunks)"
      ],
      "metadata": {
        "id": "O6jHWuckwGgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are the main methods to train an LLM without a lot of memory available?\"\n",
        "prompt_embedding = model.encode(prompt)\n",
        "similarities = util.cos_sim(prompt_embedding, chunk_embeddings)"
      ],
      "metadata": {
        "id": "KT8D3OrGwLUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most similar chunk\n",
        "most_similar_idx = similarities.argmax().item()\n",
        "most_similar_chunk = all_chunks[most_similar_idx]\n",
        "source_document = document_sources[most_similar_idx]\n",
        "\n",
        "print(f\"Prompt: {prompt}\\nMost similar chunk: {most_similar_chunk}\\nSource Document: {source_document}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8vXPWsvc7q",
        "outputId": "6f2be5ca-a7bb-435e-df72-2361d69ad046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the main methods to train an LLM without a lot of memory available?\n",
            "Most similar chunk: com/@maxshapp/understanding-and-estimating-gpu-memory-demands-for-training-llms-in-practise-c5ef20a4baff\n",
            "21 / 34Training: Mixed precision, cont\n",
            "PyTorch’s AMP: ”Some ops, like linear layers ...\n",
            "Source Document: SMU_Gen_AI_Topic_4.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOW LET'S LOOK AT USING CROSS_ENCODER https://www.sbert.net/examples/applications/cross-encoder/README.html"
      ],
      "metadata": {
        "id": "cnhStFG9pG0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "#cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')"
      ],
      "metadata": {
        "id": "SsY8iIQn_9og",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for chunk in all_chunks:\n",
        "    pairs.append([prompt, chunk])"
      ],
      "metadata": {
        "id": "Vo_7DiuLpM9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_encoder.predict(pairs)\n",
        "\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "for chunk, score in zip(all_chunks, scores):\n",
        "    print(f\"Chunk: {chunk}\\nScore: {score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqeV_imfpMw8",
        "outputId": "80c7539d-c105-49b2-d5a0-e02fb89f5b4a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the main methods to train an LLM without a lot of memory available?\n",
            "\n",
            "Chunk: Gen AI with LLMs Topic 4\n",
            "Customising LLMs Efficiently\n",
            "Laura Wynter, PhD\n",
            "SMU School of Computing and Information Sciences\n",
            "2025\n",
            "1 / 34Recall the Motivation\n",
            "▶Task-Specific objective: e.g.\n",
            "Score: -9.185474395751953\n",
            "\n",
            "Chunk: , medical or legal texts\n",
            "▶Encoders are nearly always fine-tuned for specific objectives\n",
            "▶Improved performance, for example better Q & A ability\n",
            "▶Reducing Bias\n",
            "▶Adding recent knowledge\n",
            "2 / 34Plan for Lectures on Customising LLMs part II\n",
            "▶The main libraries for LLM training, how and when to use them\n",
            "▶Mixed precision & quantisation\n",
            "▶PEFT\n",
            "▶Distributed training (and inference)\n",
            "3 / 34Main libraries for LLM training\n",
            "▶Hugging Face\n",
            "▶Torch’s FSDP\n",
            "▶DeepSpeed from Microsoft\n",
            "▶ollama (good library for group project)\n",
            "4 / 34Hugging Face\n",
            "▶Transformers is the main library within Hugging Face for training\n",
            "▶Also, accelerate for distributed training\n",
            "▶Also, PEFT for training in reduced parameter space\n",
            "▶Also, SFT for supervised fine tuning (and RL-based alignment)\n",
            "5 / 34Hugging Face Transformers\n",
            "▶We used Hugging Face Transformers already last time to train BERT\n",
            "▶Enables easily setting training hyper-parameters\n",
            "▶Hugging Face Hub stores most of the open weights LLM models available\n",
            "(and also other modalities such as audio and vision)\n",
            "▶For some uses, the pipeline() function can be helpful, but mainly for\n",
            "out-of-box experiments with no special requirements\n",
            "▶More useful is trainer class, a wrapper on pytorch trainer, and SFT trainer,\n",
            "itself a wrapper on trainer class - good combination of flexibility and\n",
            "ease-of-use\n",
            "▶Facilitates loading model, loading dataset, tokenising the data, setting\n",
            "training arguments, and setting the type of training to perform, and\n",
            "preparing data for training (batches definition and data collator)\n",
            "6 / 34Training: Reducing Exploding Gradients\n",
            "Exploding gradient refers to large increase in norm of gradient during training,\n",
            "ccauses unstable training, can lead to missing optimum\n",
            "▶L2 regularization - applies “weight decay” to loss function, reduces gradients\n",
            "by reducing weights overall\n",
            "▶Gradient clipping - clip gradients to a threshold during back prop and use\n",
            "clipped gradients to update weights\n",
            "https://arxiv.\n",
            "Score: -1.707665205001831\n",
            "\n",
            "Chunk: org/abs/1211.\n",
            "Score: -10.89268684387207\n",
            "\n",
            "Chunk: 5063\n",
            "7 / 34Training: Gradient accumulation\n",
            "”Grad accum” is a technique to enable larger batch sizes when GPU memory is\n",
            "limited by combining gradients from multiple batches before updating model\n",
            "weights\n",
            "▶Standard mini-batch training - in forward pass : a batch is fed into model,\n",
            "gradients are computed, params are updated on that single batch of data\n",
            "▶Gradient accumulation : add gradients over multiple batches : ”effective\n",
            "batch size” batch periter * iters toaccum * num procs (if distributed)\n",
            "▶Reduces amount of memory needed - larger effective batch sizes\n",
            "▶Provide more stable update direction, esp.\n",
            "Score: -2.397289276123047\n",
            "\n",
            "Chunk: with noisy gradients\n",
            "8 / 34Training: Gradient accumulation\n",
            "▶When using grad accum, adjust learning rate to ensure stable training, e.g.\n",
            "Score: -10.484641075134277\n",
            "\n",
            "Chunk: divide LR by the accumulation factor\n",
            "▶Gradient accum can impact convergence behavior - monitor training and\n",
            "lower accum iterations if convergence too slow or erratic\n",
            "▶Grad accum requires storing gradients until they are updated, this also uses\n",
            "memory\n",
            "9 / 34Training: Data parallelism\n",
            "Data parallelism is the easiest way to distribute training across multiple GPUs\n",
            "▶Break data into batches, each GPU works on one batch at a time to\n",
            "compute loss and gradients\n",
            "▶Once all GPUs finish, gradients are gathered and used to update weights.\n",
            "Score: -6.739300727844238\n",
            "\n",
            "Chunk: ..\n",
            "Score: -10.499571800231934\n",
            "\n",
            "Chunk: ▶Each GPU must have a copy of the entire network\n",
            "▶Known as DDP (for DistributedDataParallel, in pytorch)\n",
            "Source: pytorch docs\n",
            "10 / 34Training: Model Pipeline parallelism\n",
            "Model pipeline parallelism distributes training across GPUs by putting groups of\n",
            "layers on different GPUs, used when the model cannot fit on a single GPU\n",
            "▶Implement by setting desired layers .\n",
            "Score: -10.906994819641113\n",
            "\n",
            "Chunk: to() their desired device\n",
            "▶Efficient if GPUs are on same node else communication overhead is large\n",
            "▶Overhead of copying the data between devices\n",
            "▶Pipeline Parallel chunks data batch into micro-batches to allow GPUs to\n",
            "avoid waiting and work in parallel\n",
            "▶New hyper-parameter - definition of the data chunks (cf.\n",
            "Score: -11.410898208618164\n",
            "\n",
            "Chunk: grad accum)\n",
            "Image source: https://www.andrew.cmu.\n",
            "Score: -11.370086669921875\n",
            "\n",
            "Chunk: edu/course/11-667/lectures\n",
            "11 / 34Training: Model Tensor parallelism\n",
            "Split parameter tensors of target layers into different devices\n",
            "▶More efficient than pipeline parallel\n",
            "▶But more challenging to deploy as different blocks are better split differently\n",
            "▶Can be used jointly with data and pipeline parallelism\n",
            "Image source: https://www.\n",
            "Score: -11.411746978759766\n",
            "\n",
            "Chunk: andrew.cmu.\n",
            "Score: -10.963449478149414\n",
            "\n",
            "Chunk: edu/course/11-667/lectures/\n",
            "12 / 34Torch’s FSDP Distributed Training Framework\n",
            "▶FSDP means Fully Sharded Data Parallel\n",
            "▶One of the main ways to perform distributed LLM training\n",
            "▶Integrated with Hugging Face\n",
            "▶Distributed Data Parallel, or DDP training, puts a replica of the model\n",
            "weights and optim states on each GPU and sends a batch of data to each\n",
            "GPU, then uses all-reduce to sum gradients over GPU\n",
            "▶In FSDP model parameters, optimizer states and gradients are sharded in\n",
            "addition to the data batches across GPU\n",
            "▶Requires some experience to use, not out-of-the-box\n",
            "13 / 34DeepSpeed Distributed Training Framework\n",
            "▶A very early library for large-scale training and pre-training, some of whose\n",
            "innovations are also found in FSDP\n",
            "▶Also handles inference\n",
            "▶More easy to use than FSDP but de-prioritised by Microsoft, hence maybe\n",
            "not a good long term solution\n",
            "▶Hugging Face Transformers users enable DeepSpeed through –deepspeed\n",
            "flag\n",
            "14 / 34Hugging Face Accelerate\n",
            "▶Hugging Face’s library for distributed training using Hugging Face trainer\n",
            "and other libraries such as FSDP or DeepSpeed\n",
            "▶Distributed computing requires customisation as it depends on the training\n",
            "library and hardware but Accelerate does some abstraction for the user\n",
            "▶For ex.\n",
            "Score: -0.017604008316993713\n",
            "\n",
            "Chunk: setting the default config tells Accelerate to use maximum number\n",
            "of GPUs available and set mixed precision mode\n",
            "▶The accelerate config creates and saves a default config.\n",
            "Score: -11.057524681091309\n",
            "\n",
            "Chunk: yaml file in\n",
            "Accelerates cache folder\n",
            "▶Then the Accelerator class adapts to different distributed setups\n",
            "15 / 34Parameter-Efficient Fine Tuning (PEFT)\n",
            "▶PEFT relies on the observation that LLMs are heavily over-parameterised\n",
            "▶First introduced in paper ”LoRA: Low-Rank Adaptation of Large Language\n",
            "Models” from team at Microsoft Research in 2021\n",
            "▶Other methods have been introduced but not replaced LoRA, except the\n",
            "quantised version Q-LoRA, introduced in 2023 by U.\n",
            "Score: -5.808985233306885\n",
            "\n",
            "Chunk: Washington team in\n",
            "paper ”QLoRA: Efficient Finetuning of Quantized LLMs”\n",
            "▶Last lecture we trained (fine-tuned) the (small - 100m param) BERT model\n",
            "by training all of its parameters from their pre-trained values\n",
            "▶Now, using PEFT techniques, we will update (fine-tune) only a small\n",
            "number of parameters, instead of full fine tuning (FFT)\n",
            "▶With PEFT, we can train much larger models on a single GPU\n",
            "16 / 34LoRA - PEFT\n",
            "▶In FFT layer’s weight matrix W0weights updated during each iteration as\n",
            "W0x=W0x+ ∆Wx, where ∆ Wis the gradient of the weight matrix\n",
            "▶Let ∆ Wbe a product of two low-rank matrices, ∆ W=BA, hence project\n",
            "weight matrix into two low-dimensional subspace\n",
            "▶Wnew\n",
            "0x=W0x+ ∆Wx= (W0+BA)x\n",
            "17 / 34LoRA - Details\n",
            "▶”Base” (original) model weights are ”frozen” as they are not modified\n",
            "during LoRA training\n",
            "▶Update via training only the weights of the A,Bmatrices\n",
            "▶Result of LoRA training is a set of ”adapters” A,Bfor given base model\n",
            "▶(W0+BA) is called merging LoRA adapters into base weights of model\n",
            "▶Usually update around 0.\n",
            "Score: -4.914397239685059\n",
            "\n",
            "Chunk: 01% of original number of params (user choice as\n",
            "to which layers to update)\n",
            "▶LoRA approach is pluggable as updated parameters are stored separately\n",
            "from model base weights.\n",
            "Score: -11.331314086914062\n",
            "\n",
            "Chunk: LoRA ”adapters” can be shared\n",
            "18 / 34LoRA Considerations\n",
            "▶Which layers to update using LoRA training - called target modules ?\n",
            "▶What are the other hyper-parameters to determine using LoRA?\n",
            "▶r: rank of update matrices, lower rank means fewer trainable params.\n",
            "Score: -10.42348575592041\n",
            "\n",
            "Chunk: A\n",
            "usual rank for the medium-sized LLM is r=16\n",
            "▶target modules on which layers to apply LoRA matrices. One can specify to\n",
            "use all with ”all linear”\n",
            "▶loraalpha: a scaling factor.\n",
            "Score: -7.919900417327881\n",
            "\n",
            "Chunk: The actual update uses the scaling\n",
            "Wnew\n",
            "0x=W0x+α\n",
            "r(BA)x(a default is to use α=r)\n",
            "▶Other hyper-parameters are less commonly modified, kept at defaults\n",
            "19 / 34Training: Mixed precision\n",
            "The main bottleneck for LLM training and fine-tuning is GPU memory\n",
            "▶Industrial GPU like Nvidia A100 have 80 GB or 40 GB of memory, while\n",
            "consumer-grade GPU may have 24 GB memory\n",
            "▶Consider small GPT-2 model with 1.\n",
            "Score: 1.8957903385162354\n",
            "\n",
            "Chunk: 5 B params: 6 GB memory to store\n",
            "weights in 32-bit (4 bytes) FP precision, or 3 GB in 16-bit FP prec\n",
            "▶Training occupies more memory for storing tensors of model states: optim\n",
            "states, gradients, parameters, activations, and residual states: buffers\n",
            "▶Optimiser states for ADAM include momentum and variance of the gradients\n",
            "▶Mixed precision involves combining full precision (32-bit FP) with half\n",
            "precision (16-bit FP) to minimise memory while maintaining quality\n",
            "20 / 34Training: Mixed precision, cont.\n",
            "Score: -7.828153133392334\n",
            "\n",
            "Chunk: ..\n",
            "Score: -10.499571800231934\n",
            "\n",
            "Chunk: ▶Model with N params and FP16 (2 bytes) params, gradients, optimizer\n",
            "states (FP32 copy of params, momentum, and variance: 4+4+4 =12 bytes)\n",
            "▶So 16*N bytes of memory required to store model + states\n",
            "▶Also activations (cached intermediate tensors created during forward used\n",
            "for computing backward) consume significant memory, depends on layers,\n",
            "seq length, BS, hidden, heads, precision\n",
            "https://medium.\n",
            "Score: -9.693930625915527\n",
            "\n",
            "Chunk: com/@maxshapp/understanding-and-estimating-gpu-memory-demands-for-training-llms-in-practise-c5ef20a4baff\n",
            "21 / 34Training: Mixed precision, cont\n",
            "PyTorch’s AMP: ”Some ops, like linear layers ...\n",
            "Score: -0.8913214206695557\n",
            "\n",
            "Chunk: are much faster in lower\n",
            "precision. Other ops require the dynamic range of FP32. Mixed precision tries to\n",
            "match each op to its appropriate datatype.\n",
            "Score: -11.409093856811523\n",
            "\n",
            "Chunk: ”\n",
            "▶”Scaled Dot Product Attention (SDPA), when using FP16-BF16 inputs, can\n",
            "accumulate significant numerical errors due to usage of low-precision\n",
            "intermediate buffers.\n",
            "Score: -11.383176803588867\n",
            "\n",
            "Chunk: To mitigate this, default behavior is upcasting\n",
            "FP16-BF16 inputs to FP32.\n",
            "Score: -11.402524948120117\n",
            "\n",
            "Chunk: Computations are performed in FP32, and final\n",
            "FP32 results are then downcasted back to FP16-BF16”\n",
            "▶BF16 has a larger range (fewer NaNs or Infs ) with less precision than FP16\n",
            "▶BF16 is FP32 first 2 bytes, converting FP32 to BF16: discard last 2 bytes\n",
            "FP32 and vice-versa copy BF16 number into first 2 bytes of FP32 location\n",
            "https://moocaholic.\n",
            "Score: -11.315831184387207\n",
            "\n",
            "Chunk: medium.\n",
            "Score: -10.851410865783691\n",
            "\n",
            "Chunk: com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d40722 / 34Quantisation\n",
            "▶Quantisation reduces computate and memory costs by storing numbers\n",
            "using fewer bits\n",
            "▶Represent LLM paramters (weights and activations) with low-precision data\n",
            "types : 4-bit integer (int4), 8-bit integer (int8), 8-bit floating point (fp8),\n",
            "instead of full (fp32) / half (fp15) precision - less memory needed\n",
            "The section on quantisation uses images from https://newsletter.\n",
            "Score: -4.605535507202148\n",
            "\n",
            "Chunk: maartengrootendorst.com/p/a-visual-guide-to-quantization\n",
            "23 / 34Quantisation and automatic mixed-precision\n",
            "▶Mixed-precision: choosing the operations to perform in higher vs.\n",
            "Score: -11.376672744750977\n",
            "\n",
            "Chunk: lower\n",
            "precision to save memory\n",
            "▶Some libraries use the term AMP for automatic mixed precision\n",
            "▶Operations like matrix multiplies can generally be done in lower precision\n",
            "▶Norm and exp operations usually require high precision\n",
            "▶Dynamic loss scaling enables avoiding over- and underflows of gradients\n",
            "during training\n",
            "▶Scaling shifts distributions into range representable by the data type\n",
            "24 / 34Quantisation, example.\n",
            "Score: -8.191667556762695\n",
            "\n",
            "Chunk: ..\n",
            "Score: -10.499571800231934\n",
            "\n",
            "Chunk: ▶Represent LLM paramters (weights and activations) in half (fp16) precision\n",
            "▶Half-precision is a standard for computer representations, not only LLMs\n",
            "▶Can represent many numbers and operations adequately using 16 bits\n",
            "▶Representation multiplies sign (+/-), exponent and mantissa values\n",
            "Note−10= 1 and −11=−1 hence when the sign bit is 0 the number is positive and when it is 0 the number is negative\n",
            "25 / 34Quantisation, example.\n",
            "Score: -9.038206100463867\n",
            "\n",
            "Chunk: ..\n",
            "▶Compare full precision (fp32) with half-precision (fp16)\n",
            "26 / 34Quantisation, precision and dynamic range ...\n",
            "Score: -11.372048377990723\n",
            "\n",
            "Chunk: ▶Full (fp32) and half- (fp16) precision are classic IEEE standards\n",
            "▶Recently Google brain introduced BF16 (”brain float 16-bit) for LLMs\n",
            "▶BF16 has same dynamic range as fp32, with truncated precision obtained\n",
            "using more exponent bits than fp16 and fewer mantissa bits\n",
            ",\n",
            "27 / 34Quantisation, int8.\n",
            "Score: -8.073848724365234\n",
            "\n",
            "Chunk: ..\n",
            "▶Memory needed just to load model (note NVidia A100 has 80 GB of VRAM):\n",
            "▶To reduce further, use even fewer bits, e.g. int8\n",
            "28 / 34Quantisation, int8...\n",
            "Score: -11.23578929901123\n",
            "\n",
            "Chunk: ▶Dynamic range of int8 is [-127,127] so must project numbers into range\n",
            "▶Symmetric: take highest absolute value ( α) of numbers used as range of\n",
            "linear mapping from original numbers to int8 values\n",
            "▶Asymmetric: take min and max of number as new range\n",
            ",\n",
            "29 / 34De-quantisation for preserving accuracy during calculations\n",
            "▶Simple de-quantisation would add back zeros, but by storing extra data such\n",
            "as scale and zero point, a less lossy de-quant can be performed\n",
            "▶Bitsandbytes, for example, that uses int8 and int4, stores additional data\n",
            "called ”weight” and ”scb” to dequantise\n",
            "▶Some loss still occurs because quantise performs rounding that is lost when\n",
            "de-quant is performed\n",
            "▶Symmetric simple int8 quant XQ=round (X127\n",
            "max |X|)\n",
            "▶Symmetric simple int8 dequant storing max |X|XD=max |X|\n",
            "127XQ\n",
            "▶In bitsandbytes, dequantise: (weight SCB.\n",
            "Score: -11.281721115112305\n",
            "\n",
            "Chunk: unsqueeze(1) * weight)/127\n",
            "30 / 34Quantisation / de-quantisation error...\n",
            "▶Some computations should be done in higher precision to preserve accuracy\n",
            "▶Requires de-quantisation of int8 numbers, for ex.\n",
            "Score: -11.475374221801758\n",
            "\n",
            "Chunk: to fp32 / fp16 / bf16\n",
            "▶Dequantisation introduces errors, see example below\n",
            "31 / 34Quantisation, fp8...\n",
            "Score: -11.419281005859375\n",
            "\n",
            "Chunk: ▶Recently, fp8 quantisation became available in new GPUs (nvidia H100)\n",
            "▶Two variants of fp8: e4m3 and e5m2, where e5m2 is just truncated fp16\n",
            "▶Forward activations and weights require more precision, so e4m3 datatype is\n",
            "used during forward\n",
            "▶Gradients are less susceptible to loss of precision but require higher dynamic\n",
            "range so e5m2 is used to store gradients\n",
            "▶Dynamic range of fp8 is sufficient to store any activation or gradient, but\n",
            "not for all at the same time - distinct scaling factors for each fp8 tensor\n",
            "32 / 34LoRA + quantisation = qLoRA\n",
            "▶QLoRA builds on top of LoRA for further memory savings using quantisation\n",
            "▶QLoRA maintains frozen base model parameters in 4-bit integer\n",
            "▶Significantly reduced memory reduction compared to LoRA at the cost of\n",
            "longer training times and somewhat degraded quality\n",
            "▶Weights are dequantized to higher precision (bf16/fp32) during forward and\n",
            "backward computations and re-quantised back to 4-bit after computations\n",
            "▶qLoRA uses its own 4-bit data type called nf4 (normal float)\n",
            "33 / 34Quantisation for faster inference\n",
            "▶We covered the basics of quantisation with a view towards training, ex.\n",
            "Score: -8.005851745605469\n",
            "\n",
            "Chunk: using qLoRA\n",
            "▶Quantisation is also used for faster inference and smaller storage\n",
            "▶We will see these inference-related uses of quantisation (ex. GGUF,\n",
            "GPTQ,...\n",
            "Score: -11.24709701538086\n",
            "\n",
            "Chunk: ) when we cover inference later in this course\n",
            "34 / 34.\n",
            "Score: -11.435466766357422\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Assuming 'scores' and 'all_chunks' are available from the previous cell\n",
        "# Find the index of the chunk with the highest score\n",
        "best_chunk_index = np.argmax(scores)\n",
        "# Get the best chunk using the index\n",
        "best_chunk = all_chunks[best_chunk_index]\n",
        "\n",
        "# Print the best chunk\n",
        "print(f\"Best Chunk: {best_chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA5gmTX2qCKw",
        "outputId": "b4685f6a-6799-4439-d84a-b21fd639d95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Chunk: The actual update uses the scaling\n",
            "Wnew\n",
            "0x=W0x+α\n",
            "r(BA)x(a default is to use α=r)\n",
            "▶Other hyper-parameters are less commonly modified, kept at defaults\n",
            "19 / 34Training: Mixed precision\n",
            "The main bottleneck for LLM training and fine-tuning is GPU memory\n",
            "▶Industrial GPU like Nvidia A100 have 80 GB or 40 GB of memory, while\n",
            "consumer-grade GPU may have 24 GB memory\n",
            "▶Consider small GPT-2 model with 1.\n"
          ]
        }
      ]
    }
  ]
}